{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48136,"status":"ok","timestamp":1661189855776,"user":{"displayName":"胡皓量","userId":"01966224964779321794"},"user_tz":-480},"id":"YS1uxj-59UmA","outputId":"f47b72d5-57b4-4643-984f-17e928d50ddd"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 397 kB 13.6 MB/s \n","\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n","\u001b[K     |████████████████████████████████| 211 kB 59.3 MB/s \n","\u001b[K     |████████████████████████████████| 4.7 MB 56.5 MB/s \n","\u001b[K     |████████████████████████████████| 6.2 MB 22.0 MB/s \n","\u001b[K     |████████████████████████████████| 281 kB 64.4 MB/s \n","\u001b[K     |████████████████████████████████| 7.4 MB 46.4 MB/s \n","\u001b[K     |████████████████████████████████| 81 kB 10.4 MB/s \n","\u001b[K     |████████████████████████████████| 93 kB 2.0 MB/s \n","\u001b[K     |████████████████████████████████| 6.6 MB 55.6 MB/s \n","\u001b[K     |████████████████████████████████| 101 kB 12.4 MB/s \n","\u001b[?25h  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for timeout-decorator (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 165.0 MB 32 kB/s \n","\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pickle5\n","  Downloading pickle5-0.0.12-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (256 kB)\n","\u001b[K     |████████████████████████████████| 256 kB 6.8 MB/s \n","\u001b[?25hInstalling collected packages: pickle5\n","Successfully installed pickle5-0.0.12\n"]}],"source":["!pip install mumin[all]==1.6.2 torchmetrics==0.7.2 --quiet\n","!pip install dgl-cu111==0.7.2 -f https://data.dgl.ai/wheels/repo.html --quiet\n","!pip install pickle5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wtJqEQ-bsX3h"},"outputs":[],"source":["from mumin import MuminDataset\n","from google.colab import drive\n","from pathlib import Path\n","import shutil\n","import re\n","import pickle5 as pickle\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"co2X-FT-C_Ag","outputId":"a5bf03c6-06ef-4b2e-e28e-3ad7e6811743","executionInfo":{"status":"ok","timestamp":1661189934743,"user_tz":-480,"elapsed":48883,"user":{"displayName":"胡皓量","userId":"01966224964779321794"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"execute_result","data":{"text/plain":["'mumin-small.zip'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}],"source":["drive_dir = Path('drive')\n","drive.mount(str(drive_dir.resolve()), force_remount=True)\n","drive_content_dir = [child for child in drive_dir.iterdir() \n","                     if re.search(r'My ?Drive', str(child.stem)) is not None][0]\n","shutil.copy(drive_content_dir / 'mumin-small.zip', 'mumin-small.zip')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bKzCFln92Pmd"},"outputs":[],"source":["with open ('/content/drive/MyDrive/mumin-small/emb/multilingual_emb_sml_tweet.pkl', 'rb') as f:\n","    file1 = pd.DataFrame(pickle.load(f))\n","with open ('/content/drive/MyDrive/mumin-small/emb/text_emb_sml_reply.pkl', 'rb') as f:\n","    file2 = pd.DataFrame(pickle.load(f))\n","with open ('/content/drive/MyDrive/mumin-small/emb/multilingual_emb_sml_reply.pkl', 'rb') as f:\n","    file3 = pd.DataFrame(pickle.load(f))\n","with open ('/content/drive/MyDrive/mumin-small/emb/text_emb_sml_tweet.pkl', 'rb') as f:\n","    file4 = pd.DataFrame(pickle.load(f))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"elapsed":16460,"status":"ok","timestamp":1661165714768,"user":{"displayName":"胡皓量","userId":"01966224964779321794"},"user_tz":-480},"id":"eeZYAbZLu_mM","outputId":"efe1e55d-ef95-45cc-bd32-d2297e2ec0aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'mumin-small.zip'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["drive_dir = Path('drive')\n","drive.mount(str(drive_dir.resolve()), force_remount=True)\n","drive_content_dir = [child for child in drive_dir.iterdir() \n","                     if re.search(r'My ?Drive', str(child.stem)) is not None][0]\n","shutil.copy(drive_content_dir / 'mumin-small.zip', 'mumin-small.zip')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1661189947245,"user":{"displayName":"胡皓量","userId":"01966224964779321794"},"user_tz":-480},"id":"ekKc__BBRPuu","outputId":"c34bc26e-0132-42bf-a239-d10a31f755c7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["MuminDataset(size=small, compiled=False)"]},"metadata":{},"execution_count":6}],"source":["dataset = MuminDataset('mumin-small.zip')\n","dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11592,"status":"ok","timestamp":1661189958829,"user":{"displayName":"胡皓量","userId":"01966224964779321794"},"user_tz":-480},"id":"_Ow0vBqMRj5U","colab":{"base_uri":"https://localhost:8080/"},"outputId":"44e6e6d4-f284-440c-ea0f-54ba24b2727d"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:mumin.dataset:Loading dataset\n"]},{"output_type":"execute_result","data":{"text/plain":["MuminDataset(num_nodes=392,419, num_relations=483,029, size='small', compiled=True)"]},"metadata":{},"execution_count":7}],"source":["dataset.compile()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D2onJ20wyP2q"},"outputs":[],"source":["dataset.nodes['tweet']['text_emb'] = file1['mbert_emb']\n","dataset.nodes['reply']['text_emb'] = file3['mbert_emb']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mh98vmVCRabW"},"outputs":[],"source":["dataset.nodes['tweet']['mbert_emb'] = file1['mbert_mbert_auto_4']\n","dataset.nodes['tweet']['text_emb'] = file4['tweet_text_auto_64']\n","\n","dataset.nodes['reply']['mbert_emb'] = file3['mbert_auto_16']\n","dataset.nodes['reply']['text_emb'] = file2['auto_32']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Vhd7b0gTmZM"},"outputs":[],"source":["with open(\"/content/drive/MyDrive/mumin-small/emb/tweet_emb.pickle\", \"wb\") as fp:   #Pickling\n","    pickle.dump(dataset.nodes['tweet'], fp)  \n","with open(\"/content/drive/MyDrive/mumin-small/emb/reply_emb.pickle\", \"wb\") as fp:   #Pickling\n","    pickle.dump(dataset.nodes['reply'], fp)  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3976,"status":"ok","timestamp":1661117498223,"user":{"displayName":"胡皓量","userId":"01966224964779321794"},"user_tz":-480},"id":"lJBMGAOQxMxh","outputId":"bee45499-fb9f-49b1-9a14-5a22c06a1f9d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["drive_dir = Path('drive')\n","drive.mount(str(drive_dir.resolve()), force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12707,"status":"ok","timestamp":1661116887120,"user":{"displayName":"胡皓量","userId":"01966224964779321794"},"user_tz":-480},"id":"3vRRKnKjxMxh","outputId":"643a8f02-dfc2-4462-a41b-ecb135dc70a6"},"outputs":[{"data":{"text/plain":["PosixPath('drive/MyDrive/mumin-small_reduced.zip')"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["drive_content_dir = [child for child in drive_dir.iterdir() \n","                     if re.search(r'My ?Drive', str(child.stem)) is not None][0]\n","shutil.copy('mumin-small.zip', drive_content_dir / 'mumin-small_reduced.zip')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16692,"status":"ok","timestamp":1661116982111,"user":{"displayName":"胡皓量","userId":"01966224964779321794"},"user_tz":-480},"id":"QUHvRzj5oS7r","outputId":"110b9c91-5bbc-4da3-9ec9-203d12d36677"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]},{"data":{"text/plain":["PosixPath('drive/MyDrive/mumin-small_reduced.zip')"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["drive_dir = Path('drive')\n","drive_content_dir = [child for child in drive_dir.iterdir() \n","                     if re.search(r'My ?Drive', str(child.stem)) is not None][0]\n","drive.mount(str(drive_dir.resolve()), force_remount=True)\n","shutil.copy('mumin-small.zip', drive_content_dir / 'mumin-small_reduced.zip')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLffgtxfk9VQ"},"outputs":[],"source":["from PIL import Image\n","import itertools as it"]},{"cell_type":"markdown","metadata":{"id":"Vs-T5oRsPfyT"},"source":["<center><img src=\"https://filedn.com/lRBwPhPxgV74tO0rDoe8SpH/metagraph.png\" alt=\"meta graph of the MuMiN dataset\" width=\"60%\"/></center>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":350,"status":"ok","timestamp":1661107223351,"user":{"displayName":"胡皓量","userId":"01966224964779321794"},"user_tz":-480},"id":"nj-kY2uuHtuv","outputId":"1b52996c-3cf4-4dc9-9674-025ca2d9d438"},"outputs":[{"data":{"text/plain":["['claim', 'tweet', 'user', 'image', 'article', 'hashtag', 'reply']"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["list(dataset.nodes.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDsnBoO40byY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661189963530,"user_tz":-480,"elapsed":3919,"user":{"displayName":"胡皓量","userId":"01966224964779321794"}},"outputId":"b1255ce4-c5b4-42b1-ab04-3cf9003d2a74"},"outputs":[{"output_type":"stream","name":"stderr","text":["DGL backend not selected or invalid.  Assuming PyTorch for now.\n"]},{"output_type":"stream","name":"stdout","text":["Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"]},{"output_type":"stream","name":"stderr","text":["Using backend: pytorch\n"]}],"source":["from mumin import save_dgl_graph, load_dgl_graph\n","import dgl\n","import dgl.nn.pytorch as dglnn\n","import dgl.dataloading as D\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch\n","import torchmetrics as tm\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from collections import defaultdict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6107,"status":"ok","timestamp":1661189969626,"user":{"displayName":"胡皓量","userId":"01966224964779321794"},"user_tz":-480},"id":"6TFxRMIwUIck","outputId":"46d06e1e-34c9-4773-88a3-d159f78f5d79"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:mumin.dataset:Outputting to DGL\n"]},{"output_type":"execute_result","data":{"text/plain":["Graph(num_nodes={'article': 1497, 'claim': 2127, 'hashtag': 28820, 'image': 1036, 'reply': 186602, 'tweet': 4178, 'user': 158995},\n","      num_edges={('article', 'has_article_inv', 'tweet'): 1969, ('claim', 'discusses_inv', 'tweet'): 4890, ('hashtag', 'has_hashtag_inv', 'tweet'): 2343, ('hashtag', 'has_hashtag_inv', 'user'): 52420, ('image', 'has_image_inv', 'tweet'): 1045, ('reply', 'posted_inv', 'user'): 186602, ('reply', 'quote_of', 'tweet'): 93926, ('reply', 'reply_to', 'tweet'): 84271, ('tweet', 'discusses', 'claim'): 4890, ('tweet', 'has_article', 'article'): 1969, ('tweet', 'has_hashtag', 'hashtag'): 2343, ('tweet', 'has_image', 'image'): 1045, ('tweet', 'mentions', 'user'): 1134, ('tweet', 'posted_inv', 'user'): 4178, ('tweet', 'quote_of_inv', 'reply'): 93926, ('tweet', 'reply_to_inv', 'reply'): 84271, ('tweet', 'retweeted_inv', 'user'): 13710, ('user', 'follows', 'user'): 19886, ('user', 'follows_inv', 'user'): 19886, ('user', 'has_hashtag', 'hashtag'): 52420, ('user', 'mentions', 'user'): 2874, ('user', 'mentions_inv', 'tweet'): 1134, ('user', 'mentions_inv', 'user'): 2874, ('user', 'posted', 'reply'): 186602, ('user', 'posted', 'tweet'): 4178, ('user', 'retweeted', 'tweet'): 13710},\n","      metagraph=[('article', 'tweet', 'has_article_inv'), ('tweet', 'claim', 'discusses'), ('tweet', 'article', 'has_article'), ('tweet', 'hashtag', 'has_hashtag'), ('tweet', 'image', 'has_image'), ('tweet', 'user', 'mentions'), ('tweet', 'user', 'posted_inv'), ('tweet', 'user', 'retweeted_inv'), ('tweet', 'reply', 'quote_of_inv'), ('tweet', 'reply', 'reply_to_inv'), ('claim', 'tweet', 'discusses_inv'), ('hashtag', 'tweet', 'has_hashtag_inv'), ('hashtag', 'user', 'has_hashtag_inv'), ('user', 'user', 'follows'), ('user', 'user', 'follows_inv'), ('user', 'user', 'mentions'), ('user', 'user', 'mentions_inv'), ('user', 'hashtag', 'has_hashtag'), ('user', 'tweet', 'mentions_inv'), ('user', 'tweet', 'posted'), ('user', 'tweet', 'retweeted'), ('user', 'reply', 'posted'), ('image', 'tweet', 'has_image_inv'), ('reply', 'user', 'posted_inv'), ('reply', 'tweet', 'quote_of'), ('reply', 'tweet', 'reply_to')])"]},"metadata":{},"execution_count":10}],"source":["if 'dgl_graph' not in globals():\n","    dgl_graph = dataset.to_dgl()\n","dgl_graph"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1661189969627,"user":{"displayName":"胡皓量","userId":"01966224964779321794"},"user_tz":-480},"id":"4CRDS-HXT7NB","outputId":"2250df4b-1cb6-4724-ed05-e25c83b6e603"},"outputs":[{"output_type":"stream","name":"stdout","text":["Node types in the DGL graph:\n","['article', 'claim', 'hashtag', 'image', 'reply', 'tweet', 'user']\n","\n","Relation types in the DGL graph:\n"]},{"output_type":"execute_result","data":{"text/plain":["[('article', 'has_article_inv', 'tweet'),\n"," ('claim', 'discusses_inv', 'tweet'),\n"," ('hashtag', 'has_hashtag_inv', 'tweet'),\n"," ('hashtag', 'has_hashtag_inv', 'user'),\n"," ('image', 'has_image_inv', 'tweet'),\n"," ('reply', 'posted_inv', 'user'),\n"," ('reply', 'quote_of', 'tweet'),\n"," ('reply', 'reply_to', 'tweet'),\n"," ('tweet', 'discusses', 'claim'),\n"," ('tweet', 'has_article', 'article'),\n"," ('tweet', 'has_hashtag', 'hashtag'),\n"," ('tweet', 'has_image', 'image'),\n"," ('tweet', 'mentions', 'user'),\n"," ('tweet', 'posted_inv', 'user'),\n"," ('tweet', 'quote_of_inv', 'reply'),\n"," ('tweet', 'reply_to_inv', 'reply'),\n"," ('tweet', 'retweeted_inv', 'user'),\n"," ('user', 'follows', 'user'),\n"," ('user', 'follows_inv', 'user'),\n"," ('user', 'has_hashtag', 'hashtag'),\n"," ('user', 'mentions', 'user'),\n"," ('user', 'mentions_inv', 'tweet'),\n"," ('user', 'mentions_inv', 'user'),\n"," ('user', 'posted', 'reply'),\n"," ('user', 'posted', 'tweet'),\n"," ('user', 'retweeted', 'tweet')]"]},"metadata":{},"execution_count":11}],"source":["print('Node types in the DGL graph:')\n","print(dgl_graph.ntypes)\n","print('\\nRelation types in the DGL graph:')\n","dgl_graph.canonical_etypes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y1gtuiiKp5dp"},"outputs":[],"source":["plt.figure(figsize=(10, 7))\n","metagraph = dgl_graph.metagraph()\n","nx.draw_networkx(metagraph, \n","                 pos=nx.shell_layout(metagraph), \n","                 node_color='white', \n","                 node_size=3000,\n","                 arrows=False)"]},{"cell_type":"markdown","metadata":{"id":"_iAJWYu0Ibqp"},"source":["## 'user', 'posted', 'tweet'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":82595,"status":"ok","timestamp":1661176579987,"user":{"displayName":"胡皓量","userId":"01966224964779321794"},"user_tz":-480},"id":"F27oMow4Id_6","outputId":"d4523e40-3daa-4a91-cc42-de9e68c6daf2"},"outputs":[{"output_type":"stream","name":"stderr","text":["Training - loss 0.016 - factual_f1 0.978 - misinfo_f1 0.998 - val_loss 4.584 - val_factual_f1 0.103 - val_misinfo_f1 0.954: 100%|██████████| 1000/1000 [01:13<00:00, 13.57it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","*** Test results ***\n","Factual F1: 0.1690\n","Misinformation F1: 0.9305\n","Macro-average F1: 0.5498\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["rel = ('user', 'posted', 'tweet')\n","subgraph = dgl.edge_type_subgraph(dgl_graph, etypes=[rel]).to('cuda')\n","subgraph\n","train_mask = subgraph.nodes['tweet'].data['train_mask']\n","val_mask = subgraph.nodes['tweet'].data['val_mask']\n","test_mask = subgraph.nodes['tweet'].data['test_mask']\n","\n","class SAGEClassifier(nn.Module):\n","    def __init__(self, hidden_dim: int = 500):\n","        super().__init__()\n","        feats1 = subgraph.nodes[rel[0]].data['feat'].shape[-1]\n","        feats2 = subgraph.nodes[rel[2]].data['feat'].shape[-1]\n","        self.conv = dglnn.SAGEConv(in_feats=(feats1, feats2), \n","                                   out_feats=hidden_dim, \n","                                   aggregator_type='lstm',\n","                                   activation=nn.GELU())\n","        self.clf = nn.Sequential(\n","            nn.Dropout(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.Linear(hidden_dim, 1)\n","        )\n","\n","\n","    def forward(self, graph, x):\n","        x = self.conv(graph, (x['p1'], x['p2']))\n","        x = self.clf(x)\n","        return x\n","\n","gnn = SAGEClassifier().cuda()\n","gnn\n","\n","def forward_pass() -> dict:\n","    '''A forward pass of the graph neural network.\n","\n","    Returns:\n","        dict:\n","            A dict with keys 'loss', 'misinformation_f1' and 'factual_f1', \n","            with values as their corresponding values.\n","    '''\n","    # Set the GNN to training mode\n","    gnn.train()\n","\n","    # Get the input features and the output labels\n","    input_feats = dict(\n","        p1=subgraph.nodes[rel[0]].data['feat'].float().cuda(),\n","        p2=subgraph.nodes[rel[2]].data['feat'].float().cuda()\n","    )\n","    output_labels = subgraph.nodes['tweet'].data['label'].cuda()\n","\n","    # Forward propagation\n","    logits = gnn(subgraph, input_feats).squeeze()\n","\n","    # Compute loss\n","    loss = F.binary_cross_entropy_with_logits(\n","        input=logits[train_mask],\n","        target=output_labels.float()[train_mask]\n","    )\n","\n","    # Compute training metrics\n","    scores = scorer(logits[train_mask].ge(0), output_labels[train_mask])\n","    misinformation_f1 = scores[0]\n","    factual_f1 = scores[1]\n","\n","    return dict(loss=loss, \n","                misinformation_f1=misinformation_f1, \n","                factual_f1=factual_f1)\n","\n","def evaluate(split: str) -> dict:\n","    '''Evaluate the graph neural network.\n","\n","    Args:\n","        split (str):\n","            The split to evaluate the GNN on. Can be 'val' or 'test'.\n","\n","    Returns:\n","        dict:\n","            A dict with keys 'loss', 'misinformation_f1' and 'factual_f1', \n","            with values as their corresponding values.\n","    '''\n","    # Get the correct mask, depending on the value of `split`\n","    mask = val_mask if split == 'val' else test_mask\n","\n","    gnn.eval()\n","    with torch.no_grad():\n","\n","        # Get the input features and the output labels\n","        input_feats = dict(\n","            p1=subgraph.nodes[rel[0]].data['feat'].float().cuda(),\n","            p2=subgraph.nodes[rel[2]].data['feat'].float().cuda()\n","        )\n","        output_labels = subgraph.nodes['tweet'].data['label'].cuda()\n","\n","        # Forward propagation\n","        logits = gnn(subgraph, input_feats).squeeze()\n","\n","        # Compute validation loss\n","        val_loss = F.binary_cross_entropy_with_logits(\n","            input=logits[mask],\n","            target=output_labels.float()[mask]\n","        ).cpu().item()\n","\n","        # Compute validation metrics\n","        scores = scorer(logits[mask].ge(0), output_labels[mask])\n","        val_misinformation_f1 = scores[0].cpu().item()\n","        val_factual_f1 = scores[1].cpu().item()\n","\n","    return dict(loss=val_loss, \n","                misinformation_f1=val_misinformation_f1, \n","                factual_f1=val_factual_f1)\n","\n","# Initialise optimiser\n","opt = optim.AdamW(gnn.parameters(), lr=3e-4)\n","\n","# Initialise scorer\n","scorer = tm.classification.f_beta.F1Score(num_classes=2, average='none').cuda()\n","\n","# Initialise dictionary containing validation scores\n","val_scores = defaultdict(list)\n","\n","# Initialise progress bar\n","epoch_pbar = tqdm(range(1000), desc='Training')\n","\n","for epoch in epoch_pbar:\n","\n","    # Reset the gradients\n","    opt.zero_grad()\n","\n","    # Forward propagation\n","    train_results = forward_pass()\n","\n","    # Backward propagation\n","    train_results['loss'].backward()\n","\n","    # Update gradients\n","    opt.step()\n","\n","    # Evaluate the model\n","    val_results = evaluate('val')\n","\n","    # Store the validation scores\n","    for metric in ['loss', 'misinformation_f1', 'factual_f1']:\n","        val_scores[metric].append(val_results[metric])\n","\n","    # Update progress bar description\n","    if epoch % 25 == 0 and epoch > 0:\n","        val_loss = np.mean(val_scores['loss'])\n","        val_misinformation_f1 = np.mean(val_scores['misinformation_f1'])\n","        val_factual_f1 = np.mean(val_scores['factual_f1'])\n","        desc = (f'Training - '\n","                f'loss {train_results[\"loss\"]:.3f} - '\n","                f'factual_f1 {train_results[\"factual_f1\"]:.3f} - '\n","                f'misinfo_f1 {train_results[\"misinformation_f1\"]:.3f} - '\n","                f'val_loss {val_loss:.3f} - '\n","                f'val_factual_f1 {val_factual_f1:.3f} - '\n","                f'val_misinfo_f1 {val_misinformation_f1:.3f}')\n","        epoch_pbar.set_description(desc)\n","        val_scores = defaultdict(list)\n","\n","test_results = evaluate('test')\n","macro_f1 = np.mean([test_results['factual_f1'],\n","                    test_results['misinformation_f1']])\n","print()\n","print('*** Test results ***')\n","print(f'Factual F1: {test_results[\"factual_f1\"]:.4f}')\n","print(f'Misinformation F1: {test_results[\"misinformation_f1\"]:.4f}')\n","print(f'Macro-average F1: {macro_f1:.4f}')"]},{"cell_type":"markdown","metadata":{"id":"PjeuxchjCDzw"},"source":["## 'reply', 'reply_to', 'tweet'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2042029,"status":"ok","timestamp":1661185588164,"user":{"displayName":"胡皓量","userId":"01966224964779321794"},"user_tz":-480},"id":"4Et8bvPZB7oT","outputId":"2b57bb5d-7642-40a3-b5a4-bad864507e5f"},"outputs":[{"output_type":"stream","name":"stderr","text":["Training - loss 0.007 - factual_f1 0.992 - misinfo_f1 0.999 - val_loss 5.166 - val_factual_f1 0.018 - val_misinfo_f1 0.885: 100%|██████████| 1000/1000 [2:30:02<00:00,  9.00s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","*** Test results ***\n","Factual F1: 0.1880\n","Misinformation F1: 0.8817\n","Macro-average F1: 0.5349\n"]}],"source":["rel = ('reply', 'reply_to', 'tweet')\n","subgraph = dgl.edge_type_subgraph(dgl_graph, etypes=[rel]).to('cuda')\n","subgraph\n","\n","train_mask = subgraph.nodes['tweet'].data['train_mask']\n","val_mask = subgraph.nodes['tweet'].data['val_mask']\n","test_mask = subgraph.nodes['tweet'].data['test_mask']\n","\n","class SAGEClassifier(nn.Module):\n","    def __init__(self, hidden_dim: int = 500):\n","        super().__init__()\n","        feats1 = subgraph.nodes[rel[0]].data['feat'].shape[-1]\n","        feats2 = subgraph.nodes[rel[2]].data['feat'].shape[-1]\n","        self.conv = dglnn.SAGEConv(in_feats=(feats1, feats2), \n","                                   out_feats=hidden_dim, \n","                                   aggregator_type='lstm',\n","                                   activation=nn.GELU())\n","        self.clf = nn.Sequential(\n","            nn.Dropout(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.Linear(hidden_dim, 1)\n","        )\n","\n","\n","    def forward(self, graph, x):\n","        x = self.conv(graph, (x['p1'], x['p2']))\n","        x = self.clf(x)\n","        return x\n","\n","gnn = SAGEClassifier().cuda()\n","gnn\n","\n","def forward_pass() -> dict:\n","    '''A forward pass of the graph neural network.\n","\n","    Returns:\n","        dict:\n","            A dict with keys 'loss', 'misinformation_f1' and 'factual_f1', \n","            with values as their corresponding values.\n","    '''\n","    # Set the GNN to training mode\n","    gnn.train()\n","\n","    # Get the input features and the output labels\n","    input_feats = dict(\n","        p1=subgraph.nodes[rel[0]].data['feat'].float().cuda(),\n","        p2=subgraph.nodes[rel[2]].data['feat'].float().cuda()\n","    )\n","    output_labels = subgraph.nodes['tweet'].data['label'].cuda()\n","\n","    # Forward propagation\n","    logits = gnn(subgraph, input_feats).squeeze()\n","\n","    # Compute loss\n","    loss = F.binary_cross_entropy_with_logits(\n","        input=logits[train_mask],\n","        target=output_labels.float()[train_mask]\n","    )\n","\n","    # Compute training metrics\n","    scores = scorer(logits[train_mask].ge(0), output_labels[train_mask])\n","    misinformation_f1 = scores[0]\n","    factual_f1 = scores[1]\n","\n","    return dict(loss=loss, \n","                misinformation_f1=misinformation_f1, \n","                factual_f1=factual_f1)\n","\n","def evaluate(split: str) -> dict:\n","    '''Evaluate the graph neural network.\n","\n","    Args:\n","        split (str):\n","            The split to evaluate the GNN on. Can be 'val' or 'test'.\n","\n","    Returns:\n","        dict:\n","            A dict with keys 'loss', 'misinformation_f1' and 'factual_f1', \n","            with values as their corresponding values.\n","    '''\n","    # Get the correct mask, depending on the value of `split`\n","    mask = val_mask if split == 'val' else test_mask\n","\n","    gnn.eval()\n","    with torch.no_grad():\n","\n","        # Get the input features and the output labels\n","        input_feats = dict(\n","            p1=subgraph.nodes[rel[0]].data['feat'].float().cuda(),\n","            p2=subgraph.nodes[rel[2]].data['feat'].float().cuda()\n","        )\n","        output_labels = subgraph.nodes['tweet'].data['label'].cuda()\n","\n","        # Forward propagation\n","        logits = gnn(subgraph, input_feats).squeeze()\n","\n","        # Compute validation loss\n","        val_loss = F.binary_cross_entropy_with_logits(\n","            input=logits[mask],\n","            target=output_labels.float()[mask]\n","        ).cpu().item()\n","\n","        # Compute validation metrics\n","        scores = scorer(logits[mask].ge(0), output_labels[mask])\n","        val_misinformation_f1 = scores[0].cpu().item()\n","        val_factual_f1 = scores[1].cpu().item()\n","\n","    return dict(loss=val_loss, \n","                misinformation_f1=val_misinformation_f1, \n","                factual_f1=val_factual_f1)\n","\n","# Initialise optimiser\n","opt = optim.AdamW(gnn.parameters(), lr=3e-4)\n","\n","# Initialise scorer\n","scorer = tm.classification.f_beta.F1Score(num_classes=2, average='none').cuda()\n","\n","# Initialise dictionary containing validation scores\n","val_scores = defaultdict(list)\n","\n","# Initialise progress bar\n","epoch_pbar = tqdm(range(1000), desc='Training')\n","\n","for epoch in epoch_pbar:\n","\n","    # Reset the gradients\n","    opt.zero_grad()\n","\n","    # Forward propagation\n","    train_results = forward_pass()\n","\n","    # Backward propagation\n","    train_results['loss'].backward()\n","\n","    # Update gradients\n","    opt.step()\n","\n","    # Evaluate the model\n","    val_results = evaluate('val')\n","\n","    # Store the validation scores\n","    for metric in ['loss', 'misinformation_f1', 'factual_f1']:\n","        val_scores[metric].append(val_results[metric])\n","\n","    # Update progress bar description\n","    if epoch % 25 == 0 and epoch > 0:\n","        val_loss = np.mean(val_scores['loss'])\n","        val_misinformation_f1 = np.mean(val_scores['misinformation_f1'])\n","        val_factual_f1 = np.mean(val_scores['factual_f1'])\n","        desc = (f'Training - '\n","                f'loss {train_results[\"loss\"]:.3f} - '\n","                f'factual_f1 {train_results[\"factual_f1\"]:.3f} - '\n","                f'misinfo_f1 {train_results[\"misinformation_f1\"]:.3f} - '\n","                f'val_loss {val_loss:.3f} - '\n","                f'val_factual_f1 {val_factual_f1:.3f} - '\n","                f'val_misinfo_f1 {val_misinformation_f1:.3f}')\n","        epoch_pbar.set_description(desc)\n","        val_scores = defaultdict(list)\n","\n","test_results = evaluate('test')\n","macro_f1 = np.mean([test_results['factual_f1'],\n","                    test_results['misinformation_f1']])\n","print()\n","print('*** Test results ***')\n","print(f'Factual F1: {test_results[\"factual_f1\"]:.4f}')\n","print(f'Misinformation F1: {test_results[\"misinformation_f1\"]:.4f}')\n","print(f'Macro-average F1: {macro_f1:.4f}')"]},{"cell_type":"markdown","metadata":{"id":"s4t9SYsuew-1"},"source":["## 'claim', 'discusses_inv', 'tweet'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":115497,"status":"ok","timestamp":1661185703655,"user":{"displayName":"胡皓量","userId":"01966224964779321794"},"user_tz":-480},"id":"dQYv8DBYB7h3","outputId":"e0652d0c-6c7b-4c16-8cc1-3c5043b09e7f"},"outputs":[{"output_type":"stream","name":"stderr","text":["Training - loss 0.000 - factual_f1 1.000 - misinfo_f1 1.000 - val_loss 21.270 - val_factual_f1 0.022 - val_misinfo_f1 0.300: 100%|██████████| 1000/1000 [01:55<00:00,  8.66it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","*** Test results ***\n","Factual F1: 0.1066\n","Misinformation F1: 0.5258\n","Macro-average F1: 0.3162\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["rel = ('claim', 'discusses_inv', 'tweet')\n","subgraph = dgl.edge_type_subgraph(dgl_graph, etypes=[rel]).to('cuda')\n","subgraph\n","\n","train_mask = subgraph.nodes['tweet'].data['train_mask']\n","val_mask = subgraph.nodes['tweet'].data['val_mask']\n","test_mask = subgraph.nodes['tweet'].data['test_mask']\n","\n","class SAGEClassifier(nn.Module):\n","    def __init__(self, hidden_dim: int = 500):\n","        super().__init__()\n","        feats1 = subgraph.nodes[rel[0]].data['feat'].shape[-1]\n","        feats2 = subgraph.nodes[rel[2]].data['feat'].shape[-1]\n","        self.conv = dglnn.SAGEConv(in_feats=(feats1, feats2), \n","                                   out_feats=hidden_dim, \n","                                   aggregator_type='lstm',\n","                                   activation=nn.GELU())\n","        self.clf = nn.Sequential(\n","            nn.Dropout(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.Linear(hidden_dim, 1)\n","        )\n","\n","\n","    def forward(self, graph, x):\n","        x = self.conv(graph, (x['p1'], x['p2']))\n","        x = self.clf(x)\n","        return x\n","\n","gnn = SAGEClassifier().cuda()\n","gnn\n","\n","def forward_pass() -> dict:\n","    '''A forward pass of the graph neural network.\n","\n","    Returns:\n","        dict:\n","            A dict with keys 'loss', 'misinformation_f1' and 'factual_f1', \n","            with values as their corresponding values.\n","    '''\n","    # Set the GNN to training mode\n","    gnn.train()\n","\n","    # Get the input features and the output labels\n","    input_feats = dict(\n","        p1=subgraph.nodes[rel[0]].data['feat'].float().cuda(),\n","        p2=subgraph.nodes[rel[2]].data['feat'].float().cuda()\n","    )\n","    output_labels = subgraph.nodes['tweet'].data['label'].cuda()\n","\n","    # Forward propagation\n","    logits = gnn(subgraph, input_feats).squeeze()\n","\n","    # Compute loss\n","    loss = F.binary_cross_entropy_with_logits(\n","        input=logits[train_mask],\n","        target=output_labels.float()[train_mask]\n","    )\n","\n","    # Compute training metrics\n","    scores = scorer(logits[train_mask].ge(0), output_labels[train_mask])\n","    misinformation_f1 = scores[0]\n","    factual_f1 = scores[1]\n","\n","    return dict(loss=loss, \n","                misinformation_f1=misinformation_f1, \n","                factual_f1=factual_f1)\n","\n","def evaluate(split: str) -> dict:\n","    '''Evaluate the graph neural network.\n","\n","    Args:\n","        split (str):\n","            The split to evaluate the GNN on. Can be 'val' or 'test'.\n","\n","    Returns:\n","        dict:\n","            A dict with keys 'loss', 'misinformation_f1' and 'factual_f1', \n","            with values as their corresponding values.\n","    '''\n","    # Get the correct mask, depending on the value of `split`\n","    mask = val_mask if split == 'val' else test_mask\n","\n","    gnn.eval()\n","    with torch.no_grad():\n","\n","        # Get the input features and the output labels\n","        input_feats = dict(\n","            p1=subgraph.nodes[rel[0]].data['feat'].float().cuda(),\n","            p2=subgraph.nodes[rel[2]].data['feat'].float().cuda()\n","        )\n","        output_labels = subgraph.nodes['tweet'].data['label'].cuda()\n","\n","        # Forward propagation\n","        logits = gnn(subgraph, input_feats).squeeze()\n","\n","        # Compute validation loss\n","        val_loss = F.binary_cross_entropy_with_logits(\n","            input=logits[mask],\n","            target=output_labels.float()[mask]\n","        ).cpu().item()\n","\n","        # Compute validation metrics\n","        scores = scorer(logits[mask].ge(0), output_labels[mask])\n","        val_misinformation_f1 = scores[0].cpu().item()\n","        val_factual_f1 = scores[1].cpu().item()\n","\n","    return dict(loss=val_loss, \n","                misinformation_f1=val_misinformation_f1, \n","                factual_f1=val_factual_f1)\n","\n","# Initialise optimiser\n","opt = optim.AdamW(gnn.parameters(), lr=3e-4)\n","\n","# Initialise scorer\n","scorer = tm.classification.f_beta.F1Score(num_classes=2, average='none').cuda()\n","\n","# Initialise dictionary containing validation scores\n","val_scores = defaultdict(list)\n","\n","# Initialise progress bar\n","epoch_pbar = tqdm(range(1000), desc='Training')\n","\n","for epoch in epoch_pbar:\n","\n","    # Reset the gradients\n","    opt.zero_grad()\n","\n","    # Forward propagation\n","    train_results = forward_pass()\n","\n","    # Backward propagation\n","    train_results['loss'].backward()\n","\n","    # Update gradients\n","    opt.step()\n","\n","    # Evaluate the model\n","    val_results = evaluate('val')\n","\n","    # Store the validation scores\n","    for metric in ['loss', 'misinformation_f1', 'factual_f1']:\n","        val_scores[metric].append(val_results[metric])\n","\n","    # Update progress bar description\n","    if epoch % 25 == 0 and epoch > 0:\n","        val_loss = np.mean(val_scores['loss'])\n","        val_misinformation_f1 = np.mean(val_scores['misinformation_f1'])\n","        val_factual_f1 = np.mean(val_scores['factual_f1'])\n","        desc = (f'Training - '\n","                f'loss {train_results[\"loss\"]:.3f} - '\n","                f'factual_f1 {train_results[\"factual_f1\"]:.3f} - '\n","                f'misinfo_f1 {train_results[\"misinformation_f1\"]:.3f} - '\n","                f'val_loss {val_loss:.3f} - '\n","                f'val_factual_f1 {val_factual_f1:.3f} - '\n","                f'val_misinfo_f1 {val_misinformation_f1:.3f}')\n","        epoch_pbar.set_description(desc)\n","        val_scores = defaultdict(list)\n","\n","test_results = evaluate('test')\n","macro_f1 = np.mean([test_results['factual_f1'],\n","                    test_results['misinformation_f1']])\n","print()\n","print('*** Test results ***')\n","print(f'Factual F1: {test_results[\"factual_f1\"]:.4f}')\n","print(f'Misinformation F1: {test_results[\"misinformation_f1\"]:.4f}')\n","print(f'Macro-average F1: {macro_f1:.4f}')"]},{"cell_type":"markdown","metadata":{"id":"z2UQzgntfZqQ"},"source":["## 'reply', 'quote_of', 'tweet'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7850989,"status":"ok","timestamp":1661197820607,"user":{"displayName":"胡皓量","userId":"01966224964779321794"},"user_tz":-480},"id":"AyCb2KiAB7a5","outputId":"3e8e46a0-5b89-40e7-9306-98351a897d69"},"outputs":[{"output_type":"stream","name":"stderr","text":["Training - loss 0.004 - factual_f1 0.995 - misinfo_f1 0.999 - val_loss 10.060 - val_factual_f1 0.044 - val_misinfo_f1 0.842: 100%|██████████| 1000/1000 [2:10:39<00:00,  7.84s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","*** Test results ***\n","Factual F1: 0.1805\n","Misinformation F1: 0.8615\n","Macro-average F1: 0.5210\n"]}],"source":["rel = ('reply', 'quote_of', 'tweet')\n","subgraph = dgl.edge_type_subgraph(dgl_graph, etypes=[rel]).to('cuda')\n","subgraph\n","\n","train_mask = subgraph.nodes['tweet'].data['train_mask']\n","val_mask = subgraph.nodes['tweet'].data['val_mask']\n","test_mask = subgraph.nodes['tweet'].data['test_mask']\n","\n","class SAGEClassifier(nn.Module):\n","    def __init__(self, hidden_dim: int = 500):\n","        super().__init__()\n","        feats1 = subgraph.nodes[rel[0]].data['feat'].shape[-1]\n","        feats2 = subgraph.nodes[rel[2]].data['feat'].shape[-1]\n","        self.conv = dglnn.SAGEConv(in_feats=(feats1, feats2), \n","                                   out_feats=hidden_dim, \n","                                   aggregator_type='lstm',\n","                                   activation=nn.GELU())\n","        self.clf = nn.Sequential(\n","            nn.Dropout(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.Linear(hidden_dim, 1)\n","        )\n","\n","\n","    def forward(self, graph, x):\n","        x = self.conv(graph, (x['p1'], x['p2']))\n","        x = self.clf(x)\n","        return x\n","\n","gnn = SAGEClassifier().cuda()\n","gnn\n","\n","def forward_pass() -> dict:\n","    '''A forward pass of the graph neural network.\n","\n","    Returns:\n","        dict:\n","            A dict with keys 'loss', 'misinformation_f1' and 'factual_f1', \n","            with values as their corresponding values.\n","    '''\n","    # Set the GNN to training mode\n","    gnn.train()\n","\n","    # Get the input features and the output labels\n","    input_feats = dict(\n","        p1=subgraph.nodes[rel[0]].data['feat'].float().cuda(),\n","        p2=subgraph.nodes[rel[2]].data['feat'].float().cuda()\n","    )\n","    output_labels = subgraph.nodes['tweet'].data['label'].cuda()\n","\n","    # Forward propagation\n","    logits = gnn(subgraph, input_feats).squeeze()\n","\n","    # Compute loss\n","    loss = F.binary_cross_entropy_with_logits(\n","        input=logits[train_mask],\n","        target=output_labels.float()[train_mask]\n","    )\n","\n","    # Compute training metrics\n","    scores = scorer(logits[train_mask].ge(0), output_labels[train_mask])\n","    misinformation_f1 = scores[0]\n","    factual_f1 = scores[1]\n","\n","    return dict(loss=loss, \n","                misinformation_f1=misinformation_f1, \n","                factual_f1=factual_f1)\n","\n","def evaluate(split: str) -> dict:\n","    '''Evaluate the graph neural network.\n","\n","    Args:\n","        split (str):\n","            The split to evaluate the GNN on. Can be 'val' or 'test'.\n","\n","    Returns:\n","        dict:\n","            A dict with keys 'loss', 'misinformation_f1' and 'factual_f1', \n","            with values as their corresponding values.\n","    '''\n","    # Get the correct mask, depending on the value of `split`\n","    mask = val_mask if split == 'val' else test_mask\n","\n","    gnn.eval()\n","    with torch.no_grad():\n","\n","        # Get the input features and the output labels\n","        input_feats = dict(\n","            p1=subgraph.nodes[rel[0]].data['feat'].float().cuda(),\n","            p2=subgraph.nodes[rel[2]].data['feat'].float().cuda()\n","        )\n","        output_labels = subgraph.nodes['tweet'].data['label'].cuda()\n","\n","        # Forward propagation\n","        logits = gnn(subgraph, input_feats).squeeze()\n","\n","        # Compute validation loss\n","        val_loss = F.binary_cross_entropy_with_logits(\n","            input=logits[mask],\n","            target=output_labels.float()[mask]\n","        ).cpu().item()\n","\n","        # Compute validation metrics\n","        scores = scorer(logits[mask].ge(0), output_labels[mask])\n","        val_misinformation_f1 = scores[0].cpu().item()\n","        val_factual_f1 = scores[1].cpu().item()\n","\n","    return dict(loss=val_loss, \n","                misinformation_f1=val_misinformation_f1, \n","                factual_f1=val_factual_f1)\n","\n","# Initialise optimiser\n","opt = optim.AdamW(gnn.parameters(), lr=3e-4)\n","\n","# Initialise scorer\n","scorer = tm.classification.f_beta.F1Score(num_classes=2, average='none').cuda()\n","\n","# Initialise dictionary containing validation scores\n","val_scores = defaultdict(list)\n","\n","# Initialise progress bar\n","epoch_pbar = tqdm(range(1000), desc='Training')\n","\n","for epoch in epoch_pbar:\n","\n","    # Reset the gradients\n","    opt.zero_grad()\n","\n","    # Forward propagation\n","    train_results = forward_pass()\n","\n","    # Backward propagation\n","    train_results['loss'].backward()\n","\n","    # Update gradients\n","    opt.step()\n","\n","    # Evaluate the model\n","    val_results = evaluate('val')\n","\n","    # Store the validation scores\n","    for metric in ['loss', 'misinformation_f1', 'factual_f1']:\n","        val_scores[metric].append(val_results[metric])\n","\n","    # Update progress bar description\n","    if epoch % 25 == 0 and epoch > 0:\n","        val_loss = np.mean(val_scores['loss'])\n","        val_misinformation_f1 = np.mean(val_scores['misinformation_f1'])\n","        val_factual_f1 = np.mean(val_scores['factual_f1'])\n","        desc = (f'Training - '\n","                f'loss {train_results[\"loss\"]:.3f} - '\n","                f'factual_f1 {train_results[\"factual_f1\"]:.3f} - '\n","                f'misinfo_f1 {train_results[\"misinformation_f1\"]:.3f} - '\n","                f'val_loss {val_loss:.3f} - '\n","                f'val_factual_f1 {val_factual_f1:.3f} - '\n","                f'val_misinfo_f1 {val_misinformation_f1:.3f}')\n","        epoch_pbar.set_description(desc)\n","        val_scores = defaultdict(list)\n","\n","test_results = evaluate('test')\n","macro_f1 = np.mean([test_results['factual_f1'],\n","                    test_results['misinformation_f1']])\n","print()\n","print('*** Test results ***')\n","print(f'Factual F1: {test_results[\"factual_f1\"]:.4f}')\n","print(f'Misinformation F1: {test_results[\"misinformation_f1\"]:.4f}')\n","print(f'Macro-average F1: {macro_f1:.4f}')"]},{"cell_type":"markdown","metadata":{"id":"76tuRi5HfYdz"},"source":["## 'article', 'has_article_inv', 'tweet'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":85698,"status":"ok","timestamp":1661198702816,"user":{"displayName":"胡皓量","userId":"01966224964779321794"},"user_tz":-480},"id":"wLwkjCm-B7PO","outputId":"a98a648f-d048-46d4-c2a9-d37c6239a4ca"},"outputs":[{"output_type":"stream","name":"stderr","text":["Training - loss 0.026 - factual_f1 0.964 - misinfo_f1 0.996 - val_loss 0.608 - val_factual_f1 0.110 - val_misinfo_f1 0.957: 100%|██████████| 1000/1000 [01:24<00:00, 11.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","*** Test results ***\n","Factual F1: 0.2857\n","Misinformation F1: 0.9475\n","Macro-average F1: 0.6166\n"]}],"source":["rel = ('article', 'has_article_inv', 'tweet')\n","subgraph = dgl.edge_type_subgraph(dgl_graph, etypes=[rel]).to('cuda')\n","subgraph\n","\n","train_mask = subgraph.nodes['tweet'].data['train_mask']\n","val_mask = subgraph.nodes['tweet'].data['val_mask']\n","test_mask = subgraph.nodes['tweet'].data['test_mask']\n","\n","class SAGEClassifier(nn.Module):\n","    def __init__(self, hidden_dim: int = 500):\n","        super().__init__()\n","        feats1 = subgraph.nodes[rel[0]].data['feat'].shape[-1]\n","        feats2 = subgraph.nodes[rel[2]].data['feat'].shape[-1]\n","        self.conv = dglnn.SAGEConv(in_feats=(feats1, feats2), \n","                                   out_feats=hidden_dim, \n","                                   aggregator_type='lstm',\n","                                   activation=nn.GELU())\n","        self.clf = nn.Sequential(\n","            nn.Dropout(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.Linear(hidden_dim, 1)\n","        )\n","\n","\n","    def forward(self, graph, x):\n","        x = self.conv(graph, (x['p1'], x['p2']))\n","        x = self.clf(x)\n","        return x\n","\n","gnn = SAGEClassifier().cuda()\n","gnn\n","\n","def forward_pass() -> dict:\n","    '''A forward pass of the graph neural network.\n","\n","    Returns:\n","        dict:\n","            A dict with keys 'loss', 'misinformation_f1' and 'factual_f1', \n","            with values as their corresponding values.\n","    '''\n","    # Set the GNN to training mode\n","    gnn.train()\n","\n","    # Get the input features and the output labels\n","    input_feats = dict(\n","        p1=subgraph.nodes[rel[0]].data['feat'].float().cuda(),\n","        p2=subgraph.nodes[rel[2]].data['feat'].float().cuda()\n","    )\n","    output_labels = subgraph.nodes['tweet'].data['label'].cuda()\n","\n","    # Forward propagation\n","    logits = gnn(subgraph, input_feats).squeeze()\n","\n","    # Compute loss\n","    loss = F.binary_cross_entropy_with_logits(\n","        input=logits[train_mask],\n","        target=output_labels.float()[train_mask]\n","    )\n","\n","    # Compute training metrics\n","    scores = scorer(logits[train_mask].ge(0), output_labels[train_mask])\n","    misinformation_f1 = scores[0]\n","    factual_f1 = scores[1]\n","\n","    return dict(loss=loss, \n","                misinformation_f1=misinformation_f1, \n","                factual_f1=factual_f1)\n","\n","def evaluate(split: str) -> dict:\n","    '''Evaluate the graph neural network.\n","\n","    Args:\n","        split (str):\n","            The split to evaluate the GNN on. Can be 'val' or 'test'.\n","\n","    Returns:\n","        dict:\n","            A dict with keys 'loss', 'misinformation_f1' and 'factual_f1', \n","            with values as their corresponding values.\n","    '''\n","    # Get the correct mask, depending on the value of `split`\n","    mask = val_mask if split == 'val' else test_mask\n","\n","    gnn.eval()\n","    with torch.no_grad():\n","\n","        # Get the input features and the output labels\n","        input_feats = dict(\n","            p1=subgraph.nodes[rel[0]].data['feat'].float().cuda(),\n","            p2=subgraph.nodes[rel[2]].data['feat'].float().cuda()\n","        )\n","        output_labels = subgraph.nodes['tweet'].data['label'].cuda()\n","\n","        # Forward propagation\n","        logits = gnn(subgraph, input_feats).squeeze()\n","\n","        # Compute validation loss\n","        val_loss = F.binary_cross_entropy_with_logits(\n","            input=logits[mask],\n","            target=output_labels.float()[mask]\n","        ).cpu().item()\n","\n","        # Compute validation metrics\n","        scores = scorer(logits[mask].ge(0), output_labels[mask])\n","        val_misinformation_f1 = scores[0].cpu().item()\n","        val_factual_f1 = scores[1].cpu().item()\n","\n","    return dict(loss=val_loss, \n","                misinformation_f1=val_misinformation_f1, \n","                factual_f1=val_factual_f1)\n","\n","# Initialise optimiser\n","opt = optim.AdamW(gnn.parameters(), lr=3e-4)\n","\n","# Initialise scorer\n","scorer = tm.classification.f_beta.F1Score(num_classes=2, average='none').cuda()\n","\n","# Initialise dictionary containing validation scores\n","val_scores = defaultdict(list)\n","\n","# Initialise progress bar\n","epoch_pbar = tqdm(range(1000), desc='Training')\n","\n","for epoch in epoch_pbar:\n","\n","    # Reset the gradients\n","    opt.zero_grad()\n","\n","    # Forward propagation\n","    train_results = forward_pass()\n","\n","    # Backward propagation\n","    train_results['loss'].backward()\n","\n","    # Update gradients\n","    opt.step()\n","\n","    # Evaluate the model\n","    val_results = evaluate('val')\n","\n","    # Store the validation scores\n","    for metric in ['loss', 'misinformation_f1', 'factual_f1']:\n","        val_scores[metric].append(val_results[metric])\n","\n","    # Update progress bar description\n","    if epoch % 25 == 0 and epoch > 0:\n","        val_loss = np.mean(val_scores['loss'])\n","        val_misinformation_f1 = np.mean(val_scores['misinformation_f1'])\n","        val_factual_f1 = np.mean(val_scores['factual_f1'])\n","        desc = (f'Training - '\n","                f'loss {train_results[\"loss\"]:.3f} - '\n","                f'factual_f1 {train_results[\"factual_f1\"]:.3f} - '\n","                f'misinfo_f1 {train_results[\"misinformation_f1\"]:.3f} - '\n","                f'val_loss {val_loss:.3f} - '\n","                f'val_factual_f1 {val_factual_f1:.3f} - '\n","                f'val_misinfo_f1 {val_misinformation_f1:.3f}')\n","        epoch_pbar.set_description(desc)\n","        val_scores = defaultdict(list)\n","\n","test_results = evaluate('test')\n","macro_f1 = np.mean([test_results['factual_f1'],\n","                    test_results['misinformation_f1']])\n","print()\n","print('*** Test results ***')\n","print(f'Factual F1: {test_results[\"factual_f1\"]:.4f}')\n","print(f'Misinformation F1: {test_results[\"misinformation_f1\"]:.4f}')\n","print(f'Macro-average F1: {macro_f1:.4f}')"]},{"cell_type":"markdown","metadata":{"id":"FANaW6fX9co6"},"source":["## 'hashtag', 'has_hashtag_inv', 'tweet'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55588,"status":"ok","timestamp":1661198758397,"user":{"displayName":"胡皓量","userId":"01966224964779321794"},"user_tz":-480},"id":"MMjooRn3B63n","outputId":"47e7ae39-4f4e-4511-a7ac-6a359301f054"},"outputs":[{"output_type":"stream","name":"stderr","text":["Training - loss 0.033 - factual_f1 0.945 - misinfo_f1 0.994 - val_loss 0.240 - val_factual_f1 0.059 - val_misinfo_f1 0.970: 100%|██████████| 1000/1000 [00:55<00:00, 18.06it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","*** Test results ***\n","Factual F1: 0.1569\n","Misinformation F1: 0.9505\n","Macro-average F1: 0.5537\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["rel = ('hashtag', 'has_hashtag_inv', 'tweet')\n","subgraph = dgl.edge_type_subgraph(dgl_graph, etypes=[rel]).to('cuda')\n","subgraph\n","\n","train_mask = subgraph.nodes['tweet'].data['train_mask']\n","val_mask = subgraph.nodes['tweet'].data['val_mask']\n","test_mask = subgraph.nodes['tweet'].data['test_mask']\n","\n","class SAGEClassifier(nn.Module):\n","    def __init__(self, hidden_dim: int = 500):\n","        super().__init__()\n","        feats1 = subgraph.nodes[rel[0]].data['feat'].shape[-1]\n","        feats2 = subgraph.nodes[rel[2]].data['feat'].shape[-1]\n","        self.conv = dglnn.SAGEConv(in_feats=(feats1, feats2), \n","                                   out_feats=hidden_dim, \n","                                   aggregator_type='lstm',\n","                                   activation=nn.GELU())\n","        self.clf = nn.Sequential(\n","            nn.Dropout(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.Linear(hidden_dim, 1)\n","        )\n","\n","\n","    def forward(self, graph, x):\n","        x = self.conv(graph, (x['p1'], x['p2']))\n","        x = self.clf(x)\n","        return x\n","\n","gnn = SAGEClassifier().cuda()\n","gnn\n","\n","def forward_pass() -> dict:\n","    '''A forward pass of the graph neural network.\n","\n","    Returns:\n","        dict:\n","            A dict with keys 'loss', 'misinformation_f1' and 'factual_f1', \n","            with values as their corresponding values.\n","    '''\n","    # Set the GNN to training mode\n","    gnn.train()\n","\n","    # Get the input features and the output labels\n","    input_feats = dict(\n","        p1=subgraph.nodes[rel[0]].data['feat'].float().cuda(),\n","        p2=subgraph.nodes[rel[2]].data['feat'].float().cuda()\n","    )\n","    output_labels = subgraph.nodes['tweet'].data['label'].cuda()\n","\n","    # Forward propagation\n","    logits = gnn(subgraph, input_feats).squeeze()\n","\n","    # Compute loss\n","    loss = F.binary_cross_entropy_with_logits(\n","        input=logits[train_mask],\n","        target=output_labels.float()[train_mask]\n","    )\n","\n","    # Compute training metrics\n","    scores = scorer(logits[train_mask].ge(0), output_labels[train_mask])\n","    misinformation_f1 = scores[0]\n","    factual_f1 = scores[1]\n","\n","    return dict(loss=loss, \n","                misinformation_f1=misinformation_f1, \n","                factual_f1=factual_f1)\n","\n","def evaluate(split: str) -> dict:\n","    '''Evaluate the graph neural network.\n","\n","    Args:\n","        split (str):\n","            The split to evaluate the GNN on. Can be 'val' or 'test'.\n","\n","    Returns:\n","        dict:\n","            A dict with keys 'loss', 'misinformation_f1' and 'factual_f1', \n","            with values as their corresponding values.\n","    '''\n","    # Get the correct mask, depending on the value of `split`\n","    mask = val_mask if split == 'val' else test_mask\n","\n","    gnn.eval()\n","    with torch.no_grad():\n","\n","        # Get the input features and the output labels\n","        input_feats = dict(\n","            p1=subgraph.nodes[rel[0]].data['feat'].float().cuda(),\n","            p2=subgraph.nodes[rel[2]].data['feat'].float().cuda()\n","        )\n","        output_labels = subgraph.nodes['tweet'].data['label'].cuda()\n","\n","        # Forward propagation\n","        logits = gnn(subgraph, input_feats).squeeze()\n","\n","        # Compute validation loss\n","        val_loss = F.binary_cross_entropy_with_logits(\n","            input=logits[mask],\n","            target=output_labels.float()[mask]\n","        ).cpu().item()\n","\n","        # Compute validation metrics\n","        scores = scorer(logits[mask].ge(0), output_labels[mask])\n","        val_misinformation_f1 = scores[0].cpu().item()\n","        val_factual_f1 = scores[1].cpu().item()\n","\n","    return dict(loss=val_loss, \n","                misinformation_f1=val_misinformation_f1, \n","                factual_f1=val_factual_f1)\n","\n","# Initialise optimiser\n","opt = optim.AdamW(gnn.parameters(), lr=3e-4)\n","\n","# Initialise scorer\n","scorer = tm.classification.f_beta.F1Score(num_classes=2, average='none').cuda()\n","\n","# Initialise dictionary containing validation scores\n","val_scores = defaultdict(list)\n","\n","# Initialise progress bar\n","epoch_pbar = tqdm(range(1000), desc='Training')\n","\n","for epoch in epoch_pbar:\n","\n","    # Reset the gradients\n","    opt.zero_grad()\n","\n","    # Forward propagation\n","    train_results = forward_pass()\n","\n","    # Backward propagation\n","    train_results['loss'].backward()\n","\n","    # Update gradients\n","    opt.step()\n","\n","    # Evaluate the model\n","    val_results = evaluate('val')\n","\n","    # Store the validation scores\n","    for metric in ['loss', 'misinformation_f1', 'factual_f1']:\n","        val_scores[metric].append(val_results[metric])\n","\n","    # Update progress bar description\n","    if epoch % 25 == 0 and epoch > 0:\n","        val_loss = np.mean(val_scores['loss'])\n","        val_misinformation_f1 = np.mean(val_scores['misinformation_f1'])\n","        val_factual_f1 = np.mean(val_scores['factual_f1'])\n","        desc = (f'Training - '\n","                f'loss {train_results[\"loss\"]:.3f} - '\n","                f'factual_f1 {train_results[\"factual_f1\"]:.3f} - '\n","                f'misinfo_f1 {train_results[\"misinformation_f1\"]:.3f} - '\n","                f'val_loss {val_loss:.3f} - '\n","                f'val_factual_f1 {val_factual_f1:.3f} - '\n","                f'val_misinfo_f1 {val_misinformation_f1:.3f}')\n","        epoch_pbar.set_description(desc)\n","        val_scores = defaultdict(list)\n","\n","test_results = evaluate('test')\n","macro_f1 = np.mean([test_results['factual_f1'],\n","                    test_results['misinformation_f1']])\n","print()\n","print('*** Test results ***')\n","print(f'Factual F1: {test_results[\"factual_f1\"]:.4f}')\n","print(f'Misinformation F1: {test_results[\"misinformation_f1\"]:.4f}')\n","print(f'Macro-average F1: {macro_f1:.4f}')"]},{"cell_type":"markdown","metadata":{"id":"ftRPN1zQ9mbt"},"source":["## 'image', 'has_image_inv', 'tweet'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37492,"status":"ok","timestamp":1661198795877,"user":{"displayName":"胡皓量","userId":"01966224964779321794"},"user_tz":-480},"id":"b5xvAunL9lai","outputId":"6c7d06e5-c813-4f65-b62b-e2ee8f2f66a9"},"outputs":[{"output_type":"stream","name":"stderr","text":["Training - loss 0.023 - factual_f1 0.953 - misinfo_f1 0.995 - val_loss 16.370 - val_factual_f1 0.061 - val_misinfo_f1 0.875: 100%|██████████| 1000/1000 [00:37<00:00, 26.62it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","*** Test results ***\n","Factual F1: 0.1719\n","Misinformation F1: 0.8662\n","Macro-average F1: 0.5190\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["rel = ('image', 'has_image_inv', 'tweet')\n","subgraph = dgl.edge_type_subgraph(dgl_graph, etypes=[rel]).to('cuda')\n","subgraph\n","\n","train_mask = subgraph.nodes['tweet'].data['train_mask']\n","val_mask = subgraph.nodes['tweet'].data['val_mask']\n","test_mask = subgraph.nodes['tweet'].data['test_mask']\n","\n","class SAGEClassifier(nn.Module):\n","    def __init__(self, hidden_dim: int = 500):\n","        super().__init__()\n","        feats1 = subgraph.nodes[rel[0]].data['feat'].shape[-1]\n","        feats2 = subgraph.nodes[rel[2]].data['feat'].shape[-1]\n","        self.conv = dglnn.SAGEConv(in_feats=(feats1, feats2), \n","                                   out_feats=hidden_dim, \n","                                   aggregator_type='lstm',\n","                                   activation=nn.GELU())\n","        self.clf = nn.Sequential(\n","            nn.Dropout(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.Linear(hidden_dim, 1)\n","        )\n","\n","\n","    def forward(self, graph, x):\n","        x = self.conv(graph, (x['p1'], x['p2']))\n","        x = self.clf(x)\n","        return x\n","\n","gnn = SAGEClassifier().cuda()\n","gnn\n","\n","def forward_pass() -> dict:\n","    '''A forward pass of the graph neural network.\n","\n","    Returns:\n","        dict:\n","            A dict with keys 'loss', 'misinformation_f1' and 'factual_f1', \n","            with values as their corresponding values.\n","    '''\n","    # Set the GNN to training mode\n","    gnn.train()\n","\n","    # Get the input features and the output labels\n","    input_feats = dict(\n","        p1=subgraph.nodes[rel[0]].data['feat'].float().cuda(),\n","        p2=subgraph.nodes[rel[2]].data['feat'].float().cuda()\n","    )\n","    output_labels = subgraph.nodes['tweet'].data['label'].cuda()\n","\n","    # Forward propagation\n","    logits = gnn(subgraph, input_feats).squeeze()\n","\n","    # Compute loss\n","    loss = F.binary_cross_entropy_with_logits(\n","        input=logits[train_mask],\n","        target=output_labels.float()[train_mask]\n","    )\n","\n","    # Compute training metrics\n","    scores = scorer(logits[train_mask].ge(0), output_labels[train_mask])\n","    misinformation_f1 = scores[0]\n","    factual_f1 = scores[1]\n","\n","    return dict(loss=loss, \n","                misinformation_f1=misinformation_f1, \n","                factual_f1=factual_f1)\n","\n","def evaluate(split: str) -> dict:\n","    '''Evaluate the graph neural network.\n","\n","    Args:\n","        split (str):\n","            The split to evaluate the GNN on. Can be 'val' or 'test'.\n","\n","    Returns:\n","        dict:\n","            A dict with keys 'loss', 'misinformation_f1' and 'factual_f1', \n","            with values as their corresponding values.\n","    '''\n","    # Get the correct mask, depending on the value of `split`\n","    mask = val_mask if split == 'val' else test_mask\n","\n","    gnn.eval()\n","    with torch.no_grad():\n","\n","        # Get the input features and the output labels\n","        input_feats = dict(\n","            p1=subgraph.nodes[rel[0]].data['feat'].float().cuda(),\n","            p2=subgraph.nodes[rel[2]].data['feat'].float().cuda()\n","        )\n","        output_labels = subgraph.nodes['tweet'].data['label'].cuda()\n","\n","        # Forward propagation\n","        logits = gnn(subgraph, input_feats).squeeze()\n","\n","        # Compute validation loss\n","        val_loss = F.binary_cross_entropy_with_logits(\n","            input=logits[mask],\n","            target=output_labels.float()[mask]\n","        ).cpu().item()\n","\n","        # Compute validation metrics\n","        scores = scorer(logits[mask].ge(0), output_labels[mask])\n","        val_misinformation_f1 = scores[0].cpu().item()\n","        val_factual_f1 = scores[1].cpu().item()\n","\n","    return dict(loss=val_loss, \n","                misinformation_f1=val_misinformation_f1, \n","                factual_f1=val_factual_f1)\n","\n","# Initialise optimiser\n","opt = optim.AdamW(gnn.parameters(), lr=3e-4)\n","\n","# Initialise scorer\n","scorer = tm.classification.f_beta.F1Score(num_classes=2, average='none').cuda()\n","\n","# Initialise dictionary containing validation scores\n","val_scores = defaultdict(list)\n","\n","# Initialise progress bar\n","epoch_pbar = tqdm(range(1000), desc='Training')\n","\n","for epoch in epoch_pbar:\n","\n","    # Reset the gradients\n","    opt.zero_grad()\n","\n","    # Forward propagation\n","    train_results = forward_pass()\n","\n","    # Backward propagation\n","    train_results['loss'].backward()\n","\n","    # Update gradients\n","    opt.step()\n","\n","    # Evaluate the model\n","    val_results = evaluate('val')\n","\n","    # Store the validation scores\n","    for metric in ['loss', 'misinformation_f1', 'factual_f1']:\n","        val_scores[metric].append(val_results[metric])\n","\n","    # Update progress bar description\n","    if epoch % 25 == 0 and epoch > 0:\n","        val_loss = np.mean(val_scores['loss'])\n","        val_misinformation_f1 = np.mean(val_scores['misinformation_f1'])\n","        val_factual_f1 = np.mean(val_scores['factual_f1'])\n","        desc = (f'Training - '\n","                f'loss {train_results[\"loss\"]:.3f} - '\n","                f'factual_f1 {train_results[\"factual_f1\"]:.3f} - '\n","                f'misinfo_f1 {train_results[\"misinformation_f1\"]:.3f} - '\n","                f'val_loss {val_loss:.3f} - '\n","                f'val_factual_f1 {val_factual_f1:.3f} - '\n","                f'val_misinfo_f1 {val_misinformation_f1:.3f}')\n","        epoch_pbar.set_description(desc)\n","        val_scores = defaultdict(list)\n","\n","test_results = evaluate('test')\n","macro_f1 = np.mean([test_results['factual_f1'],\n","                    test_results['misinformation_f1']])\n","print()\n","print('*** Test results ***')\n","print(f'Factual F1: {test_results[\"factual_f1\"]:.4f}')\n","print(f'Misinformation F1: {test_results[\"misinformation_f1\"]:.4f}')\n","print(f'Macro-average F1: {macro_f1:.4f}')"]},{"cell_type":"markdown","metadata":{"id":"zAs3Lbih-vWH"},"source":["## 'user', 'mentions_inv', 'tweet'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75029,"status":"ok","timestamp":1661198870902,"user":{"displayName":"胡皓量","userId":"01966224964779321794"},"user_tz":-480},"id":"itTh_W40-weE","outputId":"447da0a3-436f-489d-ab05-c726805fab59"},"outputs":[{"output_type":"stream","name":"stderr","text":["Training - loss 0.021 - factual_f1 0.971 - misinfo_f1 0.997 - val_loss 4.158 - val_factual_f1 0.034 - val_misinfo_f1 0.937: 100%|██████████| 1000/1000 [01:14<00:00, 13.39it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","*** Test results ***\n","Factual F1: 0.1111\n","Misinformation F1: 0.9245\n","Macro-average F1: 0.5178\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["rel = ('user', 'mentions_inv', 'tweet')\n","subgraph = dgl.edge_type_subgraph(dgl_graph, etypes=[rel]).to('cuda')\n","subgraph\n","\n","train_mask = subgraph.nodes['tweet'].data['train_mask']\n","val_mask = subgraph.nodes['tweet'].data['val_mask']\n","test_mask = subgraph.nodes['tweet'].data['test_mask']\n","\n","class SAGEClassifier(nn.Module):\n","    def __init__(self, hidden_dim: int = 500):\n","        super().__init__()\n","        feats1 = subgraph.nodes[rel[0]].data['feat'].shape[-1]\n","        feats2 = subgraph.nodes[rel[2]].data['feat'].shape[-1]\n","        self.conv = dglnn.SAGEConv(in_feats=(feats1, feats2), \n","                                   out_feats=hidden_dim, \n","                                   aggregator_type='lstm',\n","                                   activation=nn.GELU())\n","        self.clf = nn.Sequential(\n","            nn.Dropout(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.Linear(hidden_dim, 1)\n","        )\n","\n","\n","    def forward(self, graph, x):\n","        x = self.conv(graph, (x['p1'], x['p2']))\n","        x = self.clf(x)\n","        return x\n","\n","gnn = SAGEClassifier().cuda()\n","gnn\n","\n","def forward_pass() -> dict:\n","    '''A forward pass of the graph neural network.\n","\n","    Returns:\n","        dict:\n","            A dict with keys 'loss', 'misinformation_f1' and 'factual_f1', \n","            with values as their corresponding values.\n","    '''\n","    # Set the GNN to training mode\n","    gnn.train()\n","\n","    # Get the input features and the output labels\n","    input_feats = dict(\n","        p1=subgraph.nodes[rel[0]].data['feat'].float().cuda(),\n","        p2=subgraph.nodes[rel[2]].data['feat'].float().cuda()\n","    )\n","    output_labels = subgraph.nodes['tweet'].data['label'].cuda()\n","\n","    # Forward propagation\n","    logits = gnn(subgraph, input_feats).squeeze()\n","\n","    # Compute loss\n","    loss = F.binary_cross_entropy_with_logits(\n","        input=logits[train_mask],\n","        target=output_labels.float()[train_mask]\n","    )\n","\n","    # Compute training metrics\n","    scores = scorer(logits[train_mask].ge(0), output_labels[train_mask])\n","    misinformation_f1 = scores[0]\n","    factual_f1 = scores[1]\n","\n","    return dict(loss=loss, \n","                misinformation_f1=misinformation_f1, \n","                factual_f1=factual_f1)\n","\n","def evaluate(split: str) -> dict:\n","    '''Evaluate the graph neural network.\n","\n","    Args:\n","        split (str):\n","            The split to evaluate the GNN on. Can be 'val' or 'test'.\n","\n","    Returns:\n","        dict:\n","            A dict with keys 'loss', 'misinformation_f1' and 'factual_f1', \n","            with values as their corresponding values.\n","    '''\n","    # Get the correct mask, depending on the value of `split`\n","    mask = val_mask if split == 'val' else test_mask\n","\n","    gnn.eval()\n","    with torch.no_grad():\n","\n","        # Get the input features and the output labels\n","        input_feats = dict(\n","            p1=subgraph.nodes[rel[0]].data['feat'].float().cuda(),\n","            p2=subgraph.nodes[rel[2]].data['feat'].float().cuda()\n","        )\n","        output_labels = subgraph.nodes['tweet'].data['label'].cuda()\n","\n","        # Forward propagation\n","        logits = gnn(subgraph, input_feats).squeeze()\n","\n","        # Compute validation loss\n","        val_loss = F.binary_cross_entropy_with_logits(\n","            input=logits[mask],\n","            target=output_labels.float()[mask]\n","        ).cpu().item()\n","\n","        # Compute validation metrics\n","        scores = scorer(logits[mask].ge(0), output_labels[mask])\n","        val_misinformation_f1 = scores[0].cpu().item()\n","        val_factual_f1 = scores[1].cpu().item()\n","\n","    return dict(loss=val_loss, \n","                misinformation_f1=val_misinformation_f1, \n","                factual_f1=val_factual_f1)\n","\n","# Initialise optimiser\n","opt = optim.AdamW(gnn.parameters(), lr=3e-4)\n","\n","# Initialise scorer\n","scorer = tm.classification.f_beta.F1Score(num_classes=2, average='none').cuda()\n","\n","# Initialise dictionary containing validation scores\n","val_scores = defaultdict(list)\n","\n","# Initialise progress bar\n","epoch_pbar = tqdm(range(1000), desc='Training')\n","\n","for epoch in epoch_pbar:\n","\n","    # Reset the gradients\n","    opt.zero_grad()\n","\n","    # Forward propagation\n","    train_results = forward_pass()\n","\n","    # Backward propagation\n","    train_results['loss'].backward()\n","\n","    # Update gradients\n","    opt.step()\n","\n","    # Evaluate the model\n","    val_results = evaluate('val')\n","\n","    # Store the validation scores\n","    for metric in ['loss', 'misinformation_f1', 'factual_f1']:\n","        val_scores[metric].append(val_results[metric])\n","\n","    # Update progress bar description\n","    if epoch % 25 == 0 and epoch > 0:\n","        val_loss = np.mean(val_scores['loss'])\n","        val_misinformation_f1 = np.mean(val_scores['misinformation_f1'])\n","        val_factual_f1 = np.mean(val_scores['factual_f1'])\n","        desc = (f'Training - '\n","                f'loss {train_results[\"loss\"]:.3f} - '\n","                f'factual_f1 {train_results[\"factual_f1\"]:.3f} - '\n","                f'misinfo_f1 {train_results[\"misinformation_f1\"]:.3f} - '\n","                f'val_loss {val_loss:.3f} - '\n","                f'val_factual_f1 {val_factual_f1:.3f} - '\n","                f'val_misinfo_f1 {val_misinformation_f1:.3f}')\n","        epoch_pbar.set_description(desc)\n","        val_scores = defaultdict(list)\n","\n","test_results = evaluate('test')\n","macro_f1 = np.mean([test_results['factual_f1'],\n","                    test_results['misinformation_f1']])\n","print()\n","print('*** Test results ***')\n","print(f'Factual F1: {test_results[\"factual_f1\"]:.4f}')\n","print(f'Misinformation F1: {test_results[\"misinformation_f1\"]:.4f}')\n","print(f'Macro-average F1: {macro_f1:.4f}')"]},{"cell_type":"markdown","metadata":{"id":"T017EmSv-2Rh"},"source":["## 'user', 'retweeted', 'tweet'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":232902,"status":"ok","timestamp":1661199103788,"user":{"displayName":"胡皓量","userId":"01966224964779321794"},"user_tz":-480},"id":"zKC3mUAs-6lA","outputId":"75b48fef-305c-4721-da7d-15d364253538"},"outputs":[{"output_type":"stream","name":"stderr","text":["Training - loss 0.029 - factual_f1 0.952 - misinfo_f1 0.995 - val_loss 5.427 - val_factual_f1 0.096 - val_misinfo_f1 0.944: 100%|██████████| 1000/1000 [03:52<00:00,  4.31it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","*** Test results ***\n","Factual F1: 0.2192\n","Misinformation F1: 0.9327\n","Macro-average F1: 0.5759\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["rel = ('user', 'retweeted', 'tweet')\n","subgraph = dgl.edge_type_subgraph(dgl_graph, etypes=[rel]).to('cuda')\n","subgraph\n","\n","train_mask = subgraph.nodes['tweet'].data['train_mask']\n","val_mask = subgraph.nodes['tweet'].data['val_mask']\n","test_mask = subgraph.nodes['tweet'].data['test_mask']\n","\n","class SAGEClassifier(nn.Module):\n","    def __init__(self, hidden_dim: int = 500):\n","        super().__init__()\n","        feats1 = subgraph.nodes[rel[0]].data['feat'].shape[-1]\n","        feats2 = subgraph.nodes[rel[2]].data['feat'].shape[-1]\n","        self.conv = dglnn.SAGEConv(in_feats=(feats1, feats2), \n","                                   out_feats=hidden_dim, \n","                                   aggregator_type='lstm',\n","                                   activation=nn.GELU())\n","        self.clf = nn.Sequential(\n","            nn.Dropout(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(0.2),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.Linear(hidden_dim, 1)\n","        )\n","\n","\n","    def forward(self, graph, x):\n","        x = self.conv(graph, (x['p1'], x['p2']))\n","        x = self.clf(x)\n","        return x\n","\n","gnn = SAGEClassifier().cuda()\n","gnn\n","\n","def forward_pass() -> dict:\n","    '''A forward pass of the graph neural network.\n","\n","    Returns:\n","        dict:\n","            A dict with keys 'loss', 'misinformation_f1' and 'factual_f1', \n","            with values as their corresponding values.\n","    '''\n","    # Set the GNN to training mode\n","    gnn.train()\n","\n","    # Get the input features and the output labels\n","    input_feats = dict(\n","        p1=subgraph.nodes[rel[0]].data['feat'].float().cuda(),\n","        p2=subgraph.nodes[rel[2]].data['feat'].float().cuda()\n","    )\n","    output_labels = subgraph.nodes['tweet'].data['label'].cuda()\n","\n","    # Forward propagation\n","    logits = gnn(subgraph, input_feats).squeeze()\n","\n","    # Compute loss\n","    loss = F.binary_cross_entropy_with_logits(\n","        input=logits[train_mask],\n","        target=output_labels.float()[train_mask]\n","    )\n","\n","    # Compute training metrics\n","    scores = scorer(logits[train_mask].ge(0), output_labels[train_mask])\n","    misinformation_f1 = scores[0]\n","    factual_f1 = scores[1]\n","\n","    return dict(loss=loss, \n","                misinformation_f1=misinformation_f1, \n","                factual_f1=factual_f1)\n","\n","def evaluate(split: str) -> dict:\n","    '''Evaluate the graph neural network.\n","\n","    Args:\n","        split (str):\n","            The split to evaluate the GNN on. Can be 'val' or 'test'.\n","\n","    Returns:\n","        dict:\n","            A dict with keys 'loss', 'misinformation_f1' and 'factual_f1', \n","            with values as their corresponding values.\n","    '''\n","    # Get the correct mask, depending on the value of `split`\n","    mask = val_mask if split == 'val' else test_mask\n","\n","    gnn.eval()\n","    with torch.no_grad():\n","\n","        # Get the input features and the output labels\n","        input_feats = dict(\n","            p1=subgraph.nodes[rel[0]].data['feat'].float().cuda(),\n","            p2=subgraph.nodes[rel[2]].data['feat'].float().cuda()\n","        )\n","        output_labels = subgraph.nodes['tweet'].data['label'].cuda()\n","\n","        # Forward propagation\n","        logits = gnn(subgraph, input_feats).squeeze()\n","\n","        # Compute validation loss\n","        val_loss = F.binary_cross_entropy_with_logits(\n","            input=logits[mask],\n","            target=output_labels.float()[mask]\n","        ).cpu().item()\n","\n","        # Compute validation metrics\n","        scores = scorer(logits[mask].ge(0), output_labels[mask])\n","        val_misinformation_f1 = scores[0].cpu().item()\n","        val_factual_f1 = scores[1].cpu().item()\n","\n","    return dict(loss=val_loss, \n","                misinformation_f1=val_misinformation_f1, \n","                factual_f1=val_factual_f1)\n","\n","# Initialise optimiser\n","opt = optim.AdamW(gnn.parameters(), lr=3e-4)\n","\n","# Initialise scorer\n","scorer = tm.classification.f_beta.F1Score(num_classes=2, average='none').cuda()\n","\n","# Initialise dictionary containing validation scores\n","val_scores = defaultdict(list)\n","\n","# Initialise progress bar\n","epoch_pbar = tqdm(range(1000), desc='Training')\n","\n","for epoch in epoch_pbar:\n","\n","    # Reset the gradients\n","    opt.zero_grad()\n","\n","    # Forward propagation\n","    train_results = forward_pass()\n","\n","    # Backward propagation\n","    train_results['loss'].backward()\n","\n","    # Update gradients\n","    opt.step()\n","\n","    # Evaluate the model\n","    val_results = evaluate('val')\n","\n","    # Store the validation scores\n","    for metric in ['loss', 'misinformation_f1', 'factual_f1']:\n","        val_scores[metric].append(val_results[metric])\n","\n","    # Update progress bar description\n","    if epoch % 25 == 0 and epoch > 0:\n","        val_loss = np.mean(val_scores['loss'])\n","        val_misinformation_f1 = np.mean(val_scores['misinformation_f1'])\n","        val_factual_f1 = np.mean(val_scores['factual_f1'])\n","        desc = (f'Training - '\n","                f'loss {train_results[\"loss\"]:.3f} - '\n","                f'factual_f1 {train_results[\"factual_f1\"]:.3f} - '\n","                f'misinfo_f1 {train_results[\"misinformation_f1\"]:.3f} - '\n","                f'val_loss {val_loss:.3f} - '\n","                f'val_factual_f1 {val_factual_f1:.3f} - '\n","                f'val_misinfo_f1 {val_misinformation_f1:.3f}')\n","        epoch_pbar.set_description(desc)\n","        val_scores = defaultdict(list)\n","\n","test_results = evaluate('test')\n","macro_f1 = np.mean([test_results['factual_f1'],\n","                    test_results['misinformation_f1']])\n","print()\n","print('*** Test results ***')\n","print(f'Factual F1: {test_results[\"factual_f1\"]:.4f}')\n","print(f'Misinformation F1: {test_results[\"misinformation_f1\"]:.4f}')\n","print(f'Macro-average F1: {macro_f1:.4f}')"]},{"cell_type":"markdown","metadata":{"id":"AEaVtbRy_eLD"},"source":["## performance"]},{"cell_type":"code","source":["Factual F1: 0.2192\n","Misinformation F1: 0.9327\n","Macro-average F1: 0.5759\n","232"],"metadata":{"id":"G_BY9_yWDmh_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JgY6Nb_I_E2h"},"outputs":[],"source":["model_performance = dict()\n","model_performance['user_posted_tweet'] = dict(zip(('Factual F1', 'Misinformation F1', 'Macro-average F1', 'Training Time'), (0.1690, 0.9305, 0.5498, 73)))\n","model_performance['reply_reply_to_tweet'] = dict(zip(('Factual F1', 'Misinformation F1', 'Macro-average F1', 'Training Time'), (0.1880, 0.8817, 0.5349, 9002)))\n","model_performance['claim_discusses_inv_tweet'] = dict(zip(('Factual F1', 'Misinformation F1', 'Macro-average F1', 'Training Time'), (0.1066, 0.5258, 0.3162, 115)))\n","model_performance['reply_quote_of_tweet'] = dict(zip(('Factual F1', 'Misinformation F1', 'Macro-average F1', 'Training Time'), (0.1805, 0.8615, 0.5210, 7839)))\n","model_performance['article_has_article_inv_tweet'] = dict(zip(('Factual F1', 'Misinformation F1', 'Macro-average F1', 'Training Time'), (0.2857, 0.9475, 0.6166, 84)))\n","model_performance['hashtag_has_hashtag_inv_tweet'] = dict(zip(('Factual F1', 'Misinformation F1', 'Macro-average F1', 'Training Time'), (0.1569, 0.9505, 0.5537, 55)))\n","model_performance['image_has_image_inv_tweet'] = dict(zip(('Factual F1', 'Misinformation F1', 'Macro-average F1', 'Training Time'), (0.1719, 0.8662, 0.5190, 37)))\n","model_performance['user_mentions_inv_tweet'] = dict(zip(('Factual F1', 'Misinformation F1', 'Macro-average F1', 'Training Time'), (0.1111, 0.9245, 0.5178, 74)))\n","model_performance['user_retweeted_tweet'] = dict(zip(('Factual F1', 'Misinformation F1', 'Macro-average F1', 'Training Time'), (0.2192, 0.9327, 0.5759, 232)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"elapsed":402,"status":"ok","timestamp":1660123629334,"user":{"displayName":"胡皓量","userId":"01966224964779321794"},"user_tz":-480},"id":"l6_oUO83Ap7A","outputId":"064575bd-111b-445a-9ff1-454e0ccbe383"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-c8d52e4f-341c-4e49-b51e-a0826d14aed8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Factual F1</th>\n","      <th>Misinformation F1</th>\n","      <th>Macro-average F1</th>\n","      <th>Training Time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>user_posted_tweet</th>\n","      <td>0.1633</td>\n","      <td>0.9529</td>\n","      <td>0.5581</td>\n","      <td>53.0</td>\n","    </tr>\n","    <tr>\n","      <th>reply_quote_of_tweet</th>\n","      <td>0.1791</td>\n","      <td>0.9355</td>\n","      <td>0.5573</td>\n","      <td>7804.0</td>\n","    </tr>\n","    <tr>\n","      <th>article_has_article_inv_tweet</th>\n","      <td>0.1795</td>\n","      <td>0.9240</td>\n","      <td>0.5517</td>\n","      <td>86.0</td>\n","    </tr>\n","    <tr>\n","      <th>reply_reply_to_tweet</th>\n","      <td>0.1644</td>\n","      <td>0.9280</td>\n","      <td>0.5462</td>\n","      <td>6923.0</td>\n","    </tr>\n","    <tr>\n","      <th>user_retweeted_tweet</th>\n","      <td>0.1455</td>\n","      <td>0.9457</td>\n","      <td>0.5456</td>\n","      <td>233.0</td>\n","    </tr>\n","    <tr>\n","      <th>hashtag_has_hashtag_inv_tweet</th>\n","      <td>0.0800</td>\n","      <td>0.9743</td>\n","      <td>0.5272</td>\n","      <td>60.0</td>\n","    </tr>\n","    <tr>\n","      <th>image_has_image_inv_tweet</th>\n","      <td>0.1653</td>\n","      <td>0.8736</td>\n","      <td>0.5194</td>\n","      <td>37.0</td>\n","    </tr>\n","    <tr>\n","      <th>user_mentions_inv_tweet</th>\n","      <td>0.0000</td>\n","      <td>0.9486</td>\n","      <td>0.4743</td>\n","      <td>75.0</td>\n","    </tr>\n","    <tr>\n","      <th>claim_discusses_inv_tweet</th>\n","      <td>0.1044</td>\n","      <td>0.4137</td>\n","      <td>0.2590</td>\n","      <td>92.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8d52e4f-341c-4e49-b51e-a0826d14aed8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c8d52e4f-341c-4e49-b51e-a0826d14aed8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c8d52e4f-341c-4e49-b51e-a0826d14aed8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                               Factual F1  Misinformation F1  \\\n","user_posted_tweet                  0.1633             0.9529   \n","reply_quote_of_tweet               0.1791             0.9355   \n","article_has_article_inv_tweet      0.1795             0.9240   \n","reply_reply_to_tweet               0.1644             0.9280   \n","user_retweeted_tweet               0.1455             0.9457   \n","hashtag_has_hashtag_inv_tweet      0.0800             0.9743   \n","image_has_image_inv_tweet          0.1653             0.8736   \n","user_mentions_inv_tweet            0.0000             0.9486   \n","claim_discusses_inv_tweet          0.1044             0.4137   \n","\n","                               Macro-average F1  Training Time  \n","user_posted_tweet                        0.5581           53.0  \n","reply_quote_of_tweet                     0.5573         7804.0  \n","article_has_article_inv_tweet            0.5517           86.0  \n","reply_reply_to_tweet                     0.5462         6923.0  \n","user_retweeted_tweet                     0.5456          233.0  \n","hashtag_has_hashtag_inv_tweet            0.5272           60.0  \n","image_has_image_inv_tweet                0.5194           37.0  \n","user_mentions_inv_tweet                  0.4743           75.0  \n","claim_discusses_inv_tweet                0.2590           92.0  "]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","performance = pd.DataFrame(model_performance).T\n","performance.sort_values(by=['Macro-average F1'], ascending=False)"]},{"cell_type":"code","source":["import pandas as pd\n","performance = pd.DataFrame(model_performance).T\n","performance.sort_values(by=['Macro-average F1'], ascending=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"id":"ZkzetfJBJt7n","executionInfo":{"status":"ok","timestamp":1661567697355,"user_tz":-480,"elapsed":9,"user":{"displayName":"胡皓量","userId":"01966224964779321794"}},"outputId":"48535c2d-c4a6-4207-adf0-f7a07fd7d5e6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                               Factual F1  Misinformation F1  \\\n","article_has_article_inv_tweet      0.2857             0.9475   \n","user_retweeted_tweet               0.2192             0.9327   \n","hashtag_has_hashtag_inv_tweet      0.1569             0.9505   \n","user_posted_tweet                  0.1690             0.9305   \n","reply_reply_to_tweet               0.1880             0.8817   \n","reply_quote_of_tweet               0.1805             0.8615   \n","image_has_image_inv_tweet          0.1719             0.8662   \n","user_mentions_inv_tweet            0.1111             0.9245   \n","claim_discusses_inv_tweet          0.1066             0.5258   \n","\n","                               Macro-average F1  Training Time  \n","article_has_article_inv_tweet            0.6166           84.0  \n","user_retweeted_tweet                     0.5759          232.0  \n","hashtag_has_hashtag_inv_tweet            0.5537           55.0  \n","user_posted_tweet                        0.5498           73.0  \n","reply_reply_to_tweet                     0.5349         9002.0  \n","reply_quote_of_tweet                     0.5210         7839.0  \n","image_has_image_inv_tweet                0.5190           37.0  \n","user_mentions_inv_tweet                  0.5178           74.0  \n","claim_discusses_inv_tweet                0.3162          115.0  "],"text/html":["\n","  <div id=\"df-2c208bf7-0020-47d4-a740-c32f9d5e9624\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Factual F1</th>\n","      <th>Misinformation F1</th>\n","      <th>Macro-average F1</th>\n","      <th>Training Time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>article_has_article_inv_tweet</th>\n","      <td>0.2857</td>\n","      <td>0.9475</td>\n","      <td>0.6166</td>\n","      <td>84.0</td>\n","    </tr>\n","    <tr>\n","      <th>user_retweeted_tweet</th>\n","      <td>0.2192</td>\n","      <td>0.9327</td>\n","      <td>0.5759</td>\n","      <td>232.0</td>\n","    </tr>\n","    <tr>\n","      <th>hashtag_has_hashtag_inv_tweet</th>\n","      <td>0.1569</td>\n","      <td>0.9505</td>\n","      <td>0.5537</td>\n","      <td>55.0</td>\n","    </tr>\n","    <tr>\n","      <th>user_posted_tweet</th>\n","      <td>0.1690</td>\n","      <td>0.9305</td>\n","      <td>0.5498</td>\n","      <td>73.0</td>\n","    </tr>\n","    <tr>\n","      <th>reply_reply_to_tweet</th>\n","      <td>0.1880</td>\n","      <td>0.8817</td>\n","      <td>0.5349</td>\n","      <td>9002.0</td>\n","    </tr>\n","    <tr>\n","      <th>reply_quote_of_tweet</th>\n","      <td>0.1805</td>\n","      <td>0.8615</td>\n","      <td>0.5210</td>\n","      <td>7839.0</td>\n","    </tr>\n","    <tr>\n","      <th>image_has_image_inv_tweet</th>\n","      <td>0.1719</td>\n","      <td>0.8662</td>\n","      <td>0.5190</td>\n","      <td>37.0</td>\n","    </tr>\n","    <tr>\n","      <th>user_mentions_inv_tweet</th>\n","      <td>0.1111</td>\n","      <td>0.9245</td>\n","      <td>0.5178</td>\n","      <td>74.0</td>\n","    </tr>\n","    <tr>\n","      <th>claim_discusses_inv_tweet</th>\n","      <td>0.1066</td>\n","      <td>0.5258</td>\n","      <td>0.3162</td>\n","      <td>115.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c208bf7-0020-47d4-a740-c32f9d5e9624')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2c208bf7-0020-47d4-a740-c32f9d5e9624 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2c208bf7-0020-47d4-a740-c32f9d5e9624');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}